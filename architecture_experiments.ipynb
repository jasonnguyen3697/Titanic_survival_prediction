{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model, to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import re as re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId     0.000000\n",
      "Survived        0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            19.865320\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.000000\n",
      "Cabin          77.104377\n",
      "Embarked        0.224467\n",
      "dtype: float64\n",
      "PassengerId     0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            20.574163\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.239234\n",
      "Cabin          78.229665\n",
      "Embarked        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print(train.isnull().sum()/len(train)*100)\n",
    "train.head()\n",
    "test = pd.read_csv('test.csv')\n",
    "print(test.isnull().sum()/len(test)*100)\n",
    "df = [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    S\n",
      "dtype: object\n",
      "[nan 'C85' 'C123' 'E46' 'G6' 'C103' 'D56' 'A6' 'C23 C25 C27' 'B78' 'D33'\n",
      " 'B30' 'C52' 'B28' 'C83' 'F33' 'F G73' 'E31' 'A5' 'D10 D12' 'D26' 'C110'\n",
      " 'B58 B60' 'E101' 'F E69' 'D47' 'B86' 'F2' 'C2' 'E33' 'B19' 'A7' 'C49'\n",
      " 'F4' 'A32' 'B4' 'B80' 'A31' 'D36' 'D15' 'C93' 'C78' 'D35' 'C87' 'B77'\n",
      " 'E67' 'B94' 'C125' 'C99' 'C118' 'D7' 'A19' 'B49' 'D' 'C22 C26' 'C106'\n",
      " 'C65' 'E36' 'C54' 'B57 B59 B63 B66' 'C7' 'E34' 'C32' 'B18' 'C124' 'C91'\n",
      " 'E40' 'T' 'C128' 'D37' 'B35' 'E50' 'C82' 'B96 B98' 'E10' 'E44' 'A34'\n",
      " 'C104' 'C111' 'C92' 'E38' 'D21' 'E12' 'E63' 'A14' 'B37' 'C30' 'D20' 'B79'\n",
      " 'E25' 'D46' 'B73' 'C95' 'B38' 'B39' 'B22' 'C86' 'C70' 'A16' 'C101' 'C68'\n",
      " 'A10' 'E68' 'B41' 'A20' 'D19' 'D50' 'D9' 'A23' 'B50' 'A26' 'D48' 'E58'\n",
      " 'C126' 'B71' 'B51 B53 B55' 'D49' 'B5' 'B20' 'F G63' 'C62 C64' 'E24' 'C90'\n",
      " 'C45' 'E8' 'B101' 'D45' 'C46' 'D30' 'E121' 'D11' 'E77' 'F38' 'B3' 'D6'\n",
      " 'B82 B84' 'D17' 'A36' 'B102' 'B69' 'E49' 'C47' 'D28' 'E17' 'A24' 'C50'\n",
      " 'B42' 'C148']\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "df = train\n",
    "print(df.Embarked.mode())\n",
    "Y_train = df.Survived.to_numpy().reshape((-1,1))\n",
    "print(df.Cabin.unique())\n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex       female  male\n",
      "Title                 \n",
      "Capt           0     1\n",
      "Col            0     2\n",
      "Countess       1     0\n",
      "Don            0     1\n",
      "Dr             1     6\n",
      "Jonkheer       0     1\n",
      "Lady           1     0\n",
      "Major          0     2\n",
      "Master         0    40\n",
      "Miss         182     0\n",
      "Mlle           2     0\n",
      "Mme            1     0\n",
      "Mr             0   517\n",
      "Mrs          125     0\n",
      "Ms             1     0\n",
      "Rev            0     6\n",
      "Sir            0     1\n",
      "['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'Countess' 'Jonkheer']\n"
     ]
    }
   ],
   "source": [
    "def extractTitle(name):\n",
    "    #input a name and output the title in the name\n",
    "    title = re.search(' ([A-Za-z]+)\\. ', name)\n",
    "    if title:\n",
    "        #if title found, return title\n",
    "        return title.group(1)\n",
    "    return \"No title\"\n",
    "\n",
    "df['Title'] = df['Name'].apply(extractTitle)\n",
    "test['Title'] = test['Name'].apply(extractTitle)\n",
    "\n",
    "print(pd.crosstab(df['Title'], df['Sex']))\n",
    "print(df.Title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Title = df.Title.replace(['Capt', 'Col', 'Countess', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir'], 'Rare')\n",
    "df.Title = df.Title.replace(['Mlle', 'Ms'], 'Miss')\n",
    "df.Title = df.Title.replace('Mme', 'Mrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Fill missing age\n",
    "def fillAge(df):\n",
    "    fillAge_df = df\n",
    "    \n",
    "    avgAge = fillAge_df.Age.mean()\n",
    "    stdAge = fillAge_df.Age.std()\n",
    "    ageNullCount = fillAge_df.Age.isnull().sum()\n",
    "    \n",
    "    #generate random ages based on avg and std\n",
    "    ageRandomList = np.random.randint(avgAge-stdAge, avgAge+stdAge, size=ageNullCount)\n",
    "    fillAge_df['Age'][np.isnan(fillAge_df['Age'])] = ageRandomList\n",
    "    fillAge_df['Age'] = fillAge_df['Age'].astype(int)\n",
    "    \n",
    "    return fillAge_df\n",
    "\n",
    "df = fillAge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n",
      "\n",
      "\n",
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n",
      "\n",
      "\n",
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n",
      "\n",
      "\n",
      "   CategoricalFare  Survived\n",
      "0   (-0.001, 7.91]  0.197309\n",
      "1   (7.91, 14.454]  0.303571\n",
      "2   (14.454, 31.0]  0.454955\n",
      "3  (31.0, 512.329]  0.581081\n",
      "\n",
      "\n",
      "   CategoricalAge  Survived\n",
      "0  (-0.001, 19.0]  0.460000\n",
      "1    (19.0, 25.0]  0.311765\n",
      "2    (25.0, 32.0]  0.377049\n",
      "3    (32.0, 40.0]  0.375723\n",
      "4    (40.0, 80.0]  0.381818\n",
      "\n",
      "\n",
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.336957\n",
      "\n",
      "\n",
      "   IsNotAlone  Survived\n",
      "0           0  0.303538\n",
      "1           1  0.505650\n",
      "   HasCabin  Survived\n",
      "0         0  0.299854\n",
      "1         1  0.666667\n"
     ]
    }
   ],
   "source": [
    "#print rates of survival for all data columns\n",
    "print(df[['Title', 'Survived']].groupby('Title', as_index=False).mean())\n",
    "print('\\n')\n",
    "print(df[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean())\n",
    "print('\\n')\n",
    "print(df[['Sex', 'Survived']].groupby('Sex', as_index=False).mean())\n",
    "print('\\n')\n",
    "df['CategoricalFare'] = pd.qcut(df['Fare'], 4)\n",
    "df['CategoricalAge'] = pd.qcut(df['Age'], 5)\n",
    "print(df[['CategoricalFare', 'Survived']].groupby('CategoricalFare', as_index=False).mean())\n",
    "print('\\n')\n",
    "print(df[['CategoricalAge', 'Survived']].groupby('CategoricalAge', as_index=False).mean())\n",
    "print('\\n')\n",
    "df.Embarked = df.Embarked.fillna(df.Embarked.mode())\n",
    "print(df[['Embarked', 'Survived']].groupby('Embarked', as_index=False).mean())\n",
    "print('\\n')\n",
    "df['IsNotAlone'] = 1\n",
    "df.loc[df['SibSp'] + df['Parch'] == 0, 'IsNotAlone'] = 0\n",
    "print(df[['IsNotAlone', 'Survived']].groupby('IsNotAlone', as_index=False).mean())\n",
    "df.Cabin = df.Cabin.fillna('U')\n",
    "df['HasCabin'] = 1\n",
    "df.loc[df['Cabin']=='U', 'HasCabin'] = 0\n",
    "print(df[['HasCabin', 'Survived']].groupby('HasCabin', as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Title',\n",
       "       'CategoricalFare', 'CategoricalAge', 'IsAlone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df = df.drop(['CategoricalFare', 'CategoricalAge'], axis=1)\n",
    "\n",
    "def preprocessData(df):\n",
    "    process_df = df\n",
    "    \n",
    "    #remove unnecessary columns\n",
    "    process_df = process_df.drop(['PassengerId', 'Survived', 'Name', 'Age', 'Sibsp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    #process string data into numerical data\n",
    "    #higher survival rate categories should be given higher numerical numbers to be positively correlated with survival\n",
    "    title_mapping = {'Mr':0, 'Rare':1, 'Master':2, 'Miss':3, 'Mrs':4}\n",
    "    process_df['Title'] = process_df['Title'].map(title_mapping)\n",
    "    \n",
    "    sex_mapping = {'male':0, 'female':1}\n",
    "    process_df['Sex'] = process_df['Sex'].map(sex_mapping)\n",
    "    \n",
    "    embarked_mapping = {'S':0, 'Q':1, 'C':2}\n",
    "    process_df['Embarked'] = process_df['Embarked'].map(embarked_mapping)\n",
    "    \n",
    "    process_df.loc[process_df['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    process_df.loc[process_df['Fare'] > 7.91 & process_df['Fare'] <= 14.454, 'Fare'] = 1\n",
    "    process_df.loc[process_df['Fare'] > 14.454 & process_df['Fare'] <= 31.0, 'Fare'] = 2\n",
    "    process_df.loc[process_df['Fare'] > 31.0, 'Fare'] = 3\n",
    "    \n",
    "    process_df.loc[process_df['Age'] > 19.0 & process_df['Age'] <= 25.0, 'Age'] = 0\n",
    "    process_df.loc[process_df['Age'] > 32.0 & process_df['Age'] <= 40.0, 'Age'] = 1\n",
    "    process_df.loc[process_df['Age'] > 25.0 & process_df['Age'] <= 32.0, 'Age'] = 2\n",
    "    process_df.loc[process_df['Age'] > 40.0 & process_df['Age'] <= 80.0, 'Age'] = 3\n",
    "    process_df.loc[process_df['Age'] <= 19.0, 'Age'] = 4\n",
    "    \n",
    "    return process_df\n",
    "\n",
    "df = preprocessData(df)\n",
    "test = preprocessData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(df.Survived).reshape((-1, 1))\n",
    "Y_train[:10]\n",
    "df = df.drop(['Survived', 'Pclass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforestregressor = RandomForestRegressor(max_depth=10, random_state=1, verbose=1)\n",
    "df = pd.get_dummies(df)\n",
    "randforestregressor.fit(df, train.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/\n",
    "Code for RandomForestRegressor was taken from here as well as ideas for feature selection and dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at feature importance\n",
    "features = df.columns\n",
    "importances = randforestregressor.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    # NEED TO EXPERIMENT WITH MODEL ARCHITECTURES\n",
    "    # FIRST TEST IS NUMBER OF DENSE LAYERS FROM 0 - 5\n",
    "    num_models = 6\n",
    "    model = [0] * num_models\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        inputs = Input(shape=(5,))\n",
    "        \n",
    "        X = inputs\n",
    "        \n",
    "        for j in range(i):\n",
    "            X = Dense(8, activation='relu', kernel_regularizer='l2')(X)\n",
    "        \n",
    "        outputs = Dense(1, activation='sigmoid', kernel_regularizer='l2')(X)\n",
    "        \n",
    "        model[i] = multi_gpu_model(Model(inputs=inputs, outputs=outputs), gpus=2) #initialise model with 2 GPUs\n",
    "        model[i].compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel()\n",
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.to_numpy()\n",
    "\n",
    "# REDUCE LEARNING RATE ON PLATEAU\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLY STOPPING\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=11, restore_best_weights=True, verbose=1)\n",
    "\n",
    "history = [0] * 6\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "for i in range(6):\n",
    "    history[i] = model[i].fit(x=X_train2, y=Y_train2, batch_size=128, epochs=100, callbacks=[reduce_lr], validation_data=[X_val2, Y_val2], verbose=0)\n",
    "    print(\"NN {0:d}: No. dense layers = {1:d}, Train accuracy = {2:.5f}, Validation accuracy = {3:.5f}\".format(i+1, i, max(history[i].history['acc']), max(history[i].history['val_acc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above results, 4 is the best number of dense layers. Next is experimenting with number of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel2():\n",
    "    # NEED TO EXPERIMENT WITH MODEL ARCHITECTURES\n",
    "    # SECOND TEST IS NUMBER OF NEURONS IN EACH LAYER FROM 8-128\n",
    "    num_models = 5\n",
    "    model = [0] * num_models\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        inputs = Input(shape=(5,))\n",
    "        \n",
    "        X = inputs\n",
    "        \n",
    "        for j in range(4):\n",
    "            X = Dense(8*(2**(i+j)), activation='relu', kernel_regularizer='l2')(X)\n",
    "        \n",
    "        outputs = Dense(1, activation='sigmoid', kernel_regularizer='l2')(X)\n",
    "        \n",
    "        model[i] = multi_gpu_model(Model(inputs=inputs, outputs=outputs), gpus=2) #initialise model with 2 GPUs\n",
    "        model[i].compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel2()\n",
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [0] * 5\n",
    "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "for i in range(5):\n",
    "    history[i] = model[i].fit(x=X_train2, y=Y_train2, batch_size=128, epochs=100, callbacks=[reduce_lr], validation_data=[X_val2, Y_val2], verbose=0)\n",
    "    print(\"NN {0:d}: No. nodes in layer = {1:d}, Train accuracy = {2:.5f}, Validation accuracy = {3:.5f}\".format(i+1, 8*(2**i), max(history[i].history['acc']), max(history[i].history['val_acc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall 32 nodes in each layer is the most consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel3():\n",
    "    # BUILD 15 MODELS OF ARCHITECTURE INPUT => D32 => D64 => D128 => D256 => OUTPUT\n",
    "    num_models = 15\n",
    "    model = [0] * num_models\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        inputs = Input(shape=(5,))\n",
    "        \n",
    "        X = inputs\n",
    "        \n",
    "        X = Dense(32, activation='relu', kernel_regularizer='l2')(X)\n",
    "        X = Dense(64, activation='relu', kernel_regularizer='l2')(X)\n",
    "        X = Dense(128, activation='relu', kernel_regularizer='l2')(X)\n",
    "        X = Dense(256, activation='relu', kernel_regularizer='l2')(X)\n",
    "        \n",
    "        outputs = Dense(1, activation='sigmoid')(X)\n",
    "        \n",
    "        model[i] = multi_gpu_model(Model(inputs=inputs, outputs=outputs), gpus=2) #initialise model with 2 GPUs\n",
    "        model[i].compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel3()\n",
    "model[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [0] * 15\n",
    "\n",
    "for i in range(15):\n",
    "    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "    history[i] = model[i].fit(x=X_train2, y=Y_train2, batch_size=128, epochs=100, callbacks=[reduce_lr], validation_data=[X_val2, Y_val2], verbose=0)\n",
    "    print(\"NN {0:d}: Train accuracy = {1:.5f}, Validation accuracy = {2:.5f}\".format(i+1, max(history[i].history['acc']), max(history[i].history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocess(df, meanAge):\n",
    "    preprocessed_df = df\n",
    "    preprocessed_df.Age = preprocessed_df.Age.fillna(meanAge)\n",
    "    preprocessed_df = preprocessed_df.drop(['Cabin', 'Embarked', 'Ticket', 'Name', 'PassengerId', 'Pclass'], axis=1)\n",
    "    preprocessed_df.Sex = preprocessed_df.Sex.map({'female': 1, 'male': 0})\n",
    "    \n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataPreprocess(test, meanAge)\n",
    "X_test = test_df.to_numpy()\n",
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "results = np.zeros((X_test.shape[0],1))\n",
    "for i in range(15):\n",
    "    results = results + model[i].predict(X_test)\n",
    "results = pd.Series(np.rint(results/15).astype(int).reshape(X_test.shape[0]), name='Survived')\n",
    "passengerid = test.PassengerId\n",
    "submission = pd.concat([passengerid, results], axis=1)\n",
    "submission.to_csv('ENSEMBLE_DENSE_L2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
